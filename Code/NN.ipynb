{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, balanced_accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        mat_data = loadmat(data_path)\n",
    "        data = mat_data['data']\n",
    "        self.data = data[:, 0:272]\n",
    "        self.labels = data[:, 272:273]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.from_numpy(sample).float(), torch.tensor(label).long()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(data_path=\"./your_dataset\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN\n",
    "1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.1):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)  \n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  \n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(data_path=\"./your_dataset\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "accuracies, recalls, precisions, f1s, balanced_accs, unweighted_f1s = [], [], [], [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(dataset):\n",
    "\n",
    "    train_dataset = Subset(dataset, train_index)\n",
    "    test_dataset = Subset(dataset, test_index)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = SimpleNN(input_size=272, hidden_size=100, num_classes=3, dropout_rate=0.1).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    for epoch in range(30):  \n",
    "        for data, label in train_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_pred, y_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred += predicted.tolist()\n",
    "            y_test += label.tolist()\n",
    "\n",
    "    \n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    precisions.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    balanced_accs.append(balanced_accuracy_score(y_test, y_pred))\n",
    "    unweighted_f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "\n",
    "metrics1 = {\n",
    "    'Accuracy': (np.mean(accuracies), np.std(accuracies)),\n",
    "    'Recall': (np.mean(recalls), np.std(recalls)),\n",
    "    'Precision': (np.mean(precisions), np.std(precisions)),\n",
    "    'F1 Score': (np.mean(f1s), np.std(f1s)),\n",
    "    'Balanced Accuracy': (np.mean(balanced_accs), np.std(balanced_accs)),\n",
    "    'Unweighted F1 Score': (np.mean(unweighted_f1s), np.std(unweighted_f1s))\n",
    "}\n",
    "\n",
    "for metric, values in metrics1.items():\n",
    "    print(f\"{metric} - Mean: {values[0]}, Standard Deviation: {values[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN1 5 layers\n",
    "5 hidden layers batchnorm dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
    "        super(ComplexNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn5 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout5 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc6 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(data_path=\"./your_dataset\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "accuracies, recalls, precisions, f1s, balanced_accs, unweighted_f1s = [], [], [], [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(dataset):\n",
    "\n",
    "    train_dataset = Subset(dataset, train_index)\n",
    "    test_dataset = Subset(dataset, test_index)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = ComplexNN(input_size=272, hidden_size=100, num_classes=3, dropout_rate=0.1).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(33):  \n",
    "        for data, label in train_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred, y_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred += predicted.tolist()\n",
    "            y_test += label.tolist()\n",
    "\n",
    "    \n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    precisions.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    balanced_accs.append(balanced_accuracy_score(y_test, y_pred))\n",
    "    unweighted_f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "metrics2 = {\n",
    "    'Accuracy': (np.mean(accuracies), np.std(accuracies)),\n",
    "    'Recall': (np.mean(recalls), np.std(recalls)),\n",
    "    'Precision': (np.mean(precisions), np.std(precisions)),\n",
    "    'F1 Score': (np.mean(f1s), np.std(f1s)),\n",
    "    'Balanced Accuracy': (np.mean(balanced_accs), np.std(balanced_accs)),\n",
    "    'Unweighted F1 Score': (np.mean(unweighted_f1s), np.std(unweighted_f1s))\n",
    "}\n",
    "\n",
    "\n",
    "for metric, values in metrics2.items():\n",
    "    print(f\"{metric} - Mean: {values[0]}, Standard Deviation: {values[1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN2 residual 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualComplexNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
    "        super(ResidualComplexNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        \n",
    "        self.fc5 = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.bn5 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout5 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc6 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.bn1(self.fc1(x)))\n",
    "        x1 = self.dropout1(x1)\n",
    "\n",
    "        x2 = self.relu(self.bn2(self.fc2(x1)))\n",
    "        x2 = self.dropout2(x2)\n",
    "\n",
    "        x3 = self.relu(self.bn3(self.fc3(x2)))\n",
    "        x3 = self.dropout3(x3)\n",
    "\n",
    "        x4 = self.relu(self.bn4(self.fc4(x3)))\n",
    "        x4 = self.dropout4(x4)\n",
    "\n",
    "        \n",
    "        x4 = torch.cat((x1, x4), dim=1)\n",
    "\n",
    "        x5 = self.relu(self.bn5(self.fc5(x4)))\n",
    "        x5 = self.dropout5(x5)\n",
    "\n",
    "        x6 = self.fc6(x5)\n",
    "        return x6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(data_path=\"./your_dataset\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "accuracies, recalls, precisions, f1s, balanced_accs, unweighted_f1s = [], [], [], [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(dataset):\n",
    "\n",
    "    train_dataset = Subset(dataset, train_index)\n",
    "    test_dataset = Subset(dataset, test_index)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = ResidualComplexNN(input_size=272, hidden_size=100, num_classes=3, dropout_rate=0.1).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(30): \n",
    "        for data, label in train_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_pred, y_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred += predicted.tolist()\n",
    "            y_test += label.tolist()\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    precisions.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    balanced_accs.append(balanced_accuracy_score(y_test, y_pred))\n",
    "    unweighted_f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "metrics3 = {\n",
    "    'Accuracy': (np.mean(accuracies), np.std(accuracies)),\n",
    "    'Recall': (np.mean(recalls), np.std(recalls)),\n",
    "    'Precision': (np.mean(precisions), np.std(precisions)),\n",
    "    'F1 Score': (np.mean(f1s), np.std(f1s)),\n",
    "    'Balanced Accuracy': (np.mean(balanced_accs), np.std(balanced_accs)),\n",
    "    'Unweighted F1 Score': (np.mean(unweighted_f1s), np.std(unweighted_f1s))\n",
    "}\n",
    "\n",
    "\n",
    "for metric, values in metrics3.items():\n",
    "    print(f\"{metric} - Mean: {values[0]}, Standard Deviation: {values[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN3 residual 9 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualResidualNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
    "        super(DualResidualNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        \n",
    "        self.fc5 = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.bn5 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout5 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        \n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn6 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout6 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn7 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout7 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn8 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout8 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        \n",
    "        self.fc9 = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.bn9 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout9 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        residual1 = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(residual1)\n",
    "\n",
    "        \n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        \n",
    "        x = torch.cat((residual1, x), dim=1)\n",
    "        x = self.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        residual2 = x\n",
    "\n",
    "        x = self.relu(self.bn6(self.fc6(x)))\n",
    "        x = self.dropout6(x)\n",
    "        x = self.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.dropout7(x)\n",
    "        x = self.relu(self.bn8(self.fc8(x)))\n",
    "        x = self.dropout8(x)\n",
    "\n",
    "        x = torch.cat((residual2, x), dim=1)\n",
    "        x = self.relu(self.bn9(self.fc9(x)))\n",
    "        x = self.dropout9(x)\n",
    "\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(data_path=\"./your_dataset\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "\n",
    "accuracies, recalls, precisions, f1s, balanced_accs, unweighted_f1s = [], [], [], [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(dataset):\n",
    "\n",
    "    train_dataset = Subset(dataset, train_index)\n",
    "    test_dataset = Subset(dataset, test_index)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "    model = DualResidualNN(input_size=272, hidden_size=100, num_classes=3, dropout_rate=0.1).to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    for epoch in range(35):  \n",
    "        for data, label in train_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred, y_test = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred += predicted.tolist()\n",
    "            y_test += label.tolist()\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    precisions.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    balanced_accs.append(balanced_accuracy_score(y_test, y_pred))\n",
    "    unweighted_f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "metrics4 = {\n",
    "    'Accuracy': (np.mean(accuracies), np.std(accuracies)),\n",
    "    'Recall': (np.mean(recalls), np.std(recalls)),\n",
    "    'Precision': (np.mean(precisions), np.std(precisions)),\n",
    "    'F1 Score': (np.mean(f1s), np.std(f1s)),\n",
    "    'Balanced Accuracy': (np.mean(balanced_accs), np.std(balanced_accs)),\n",
    "    'Unweighted F1 Score': (np.mean(unweighted_f1s), np.std(unweighted_f1s))\n",
    "}\n",
    "\n",
    "\n",
    "for metric, values in metrics4.items():\n",
    "    print(f\"{metric} - Mean: {values[0]}, Standard Deviation: {values[1]}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
